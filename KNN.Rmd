---
title: "K Nearest Neighbors"
author: "George M. Ndede"
date: "12/01/2021"
output: html_document
---

```{r setup_knn, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Predict Attrition with K-nearest-neighbor algorithm.

## Define functions and select predictors

```{r}
library(caret)
library(class)
```

```{r prep_for_knn}
# scale all numeric variables to normalize
# then select only the variable to be predicted and the normalized values
for_knn <- employee_data %>%
  dplyr::select(Attrition,
         z_DistanceFromHome,
         z_Education,
         z_EnvironmentSatisfaction,
         z_JobInvolvement,
         z_JobLevel,
         z_JobSatisfaction,
         z_NumCompaniesWorked,
         z_PercentSalaryHike,
         z_PerformanceRating,
         z_RelationshipSatisfaction,
         z_StockOptionLevel,
         z_TotalWorkingYears,
         z_TrainingTimesLastYear,
         z_WorkLifeBalance,
         z_YearsAtCompany,
         z_YearsInCurrentRole,
         z_YearsSinceLastPromotion,
         z_YearsWithCurrManager,
         z_OverTime,
         z_MaritalStatus)

yes_obs <- for_knn %>%
  filter(Attrition == 'Yes')

no_obs <- for_knn %>%
  filter(Attrition == 'No')

cols_for_knn <- c("z_DistanceFromHome",
                   "z_EnvironmentSatisfaction",
                   "z_NumCompaniesWorked",
                   "z_TotalWorkingYears",
                   "z_TrainingTimesLastYear",
                   "z_WorkLifeBalance",
                   "z_YearsSinceLastPromotion",
                   "z_JobInvolvement",
                   "z_JobSatisfaction",
                   "z_MaritalStatus",
                   "z_OverTime")

# adjust the knn results by known probabilities
knn_adjust <- function(orig_factor, k, num_no=1, num_yes=1)
{
  # What was original answer
  orig_answer <- as.character(orig_factor)
  other_answer <- ifelse(orig_answer == 'No','Yes','No')
  
  # original probabilities
  orig_win    <- attr(orig_factor,'prob')
  orig_lose   <- 1.0 - orig_win

  # determine how many votes for winner and loser  
  win_votes <- k * orig_win
  lose_votes <- k - win_votes
  
  # the next two statements normalize votes 
  # by the total available no or yes observations
  
  # if the original answer was no, then divide win by total number of no's
  win  <- win_votes  / ifelse (orig_answer == 'No', num_no, num_yes)
  
  # if the original answer was yes, then loser was 'no' ...
  lose <- lose_votes / ifelse (orig_answer == 'Yes',num_no, num_yes)
  
  # alter the answer if lose > win
  new_answer <- as.factor(ifelse( (lose > win), other_answer, orig_answer))
  
  #cat('--- rmp debug ---')
  #cat(' k          ',k,'\n')
  #cat(' num_no     ',num_no,'\n')
  #cat(' num_yes    ',num_yes,'\n')
  #cat(' orig_answer',orig_answer,'\n')
  #cat(' orig_win   ',orig_win,'\n')
  #cat(' win_votes  ',win_votes,'\n')
  #cat(' (new) win  ',win,'\n')
  #cat(' (new) lose ',lose,'\n')
  #cat(' new_answer ',new_answer,'\n')
  
  levels(new_answer) <- levels(orig_factor)
  return(new_answer)
}

         
```

## What is the expected outcome?

```{r knn_start}
summary(for_knn$Attrition)
```

```{r knn_sanity}
truth <- dplyr::count(employee_data,vars=Attrition)
truth.yes <- as.numeric(truth[2,2])
truth.no  <- as.numeric(truth[1,2])
truth.nobs <- truth.yes + truth.no
true_yes_perc <- round(100*truth.yes/truth.nobs)
true_no_perc  <- round(100*truth.no/truth.nobs)
```

True percentage of "yes" values is `r true_yes_perc`  
True percentage of "no"  values is `r true_no_perc` 

## Set parameters for KNN

```{r knn_parms}
set.seed(1)
train_frac <- 0.7
numks <- 50
knn_trials <- 250
positive_value <- 'Yes'
max_acc_k = 10
chosen_k = 5
```

## Select K

run KNN with different values of K to select appropriate neighborhood size.

```{r do_knn}
# look for the best value of k
#  do several trials, and for each trial
#   split data into training and test
#   make classifier and from the confusion matrix,
#   save the accuracy, sensitivity, and specificity
knn_stats = matrix(nrow=numks,ncol=6)
true_pcyes <- truth.yes/truth.nobs
for (i in 1:numks)
{
  acc  = matrix(nrow=knn_trials,ncol=1)
  sens = matrix(nrow=knn_trials,ncol=1)
  spec = matrix(nrow=knn_trials,ncol=1)
  pcyes = matrix(nrow=knn_trials,ncol=1)
  pcno  = matrix(nrow=knn_trials,ncol=1)
  for (j in 1:knn_trials)
  {
    # extract equal 'Yes' and 'No' subsets, put together into one set, which 
    # will then be split into training and test parts
    cnt <- min(truth.yes,truth.no)
    trial_space <- rbind(sample_n(yes_obs,cnt,replace=FALSE),sample_n(no_obs,cnt,replace=FALSE))
    
    trainIndices = sample(1:dim(trial_space)[1],round(train_frac * dim(trial_space)[1]))
    trainData = trial_space[trainIndices,]
    testData = trial_space[-trainIndices,]
    raw_class = knn(trainData[,cols_for_knn],
                              testData[,cols_for_knn],
                              trainData$Attrition, prob = TRUE, k = i)
    
    # with the equal sampling, there should be no need to normalize
    # classifications = knn_adjust(raw_class, k = i, num_no = truth.no, num_yes = truth.yes)
    classifications <- raw_class
    tmp <- data.frame(classifications)
    answers <- count(tmp,vars=classifications)
    
    num_yes <- as.numeric(answers[2,2])
    num_no  <- as.numeric(answers[1,2])
    num_obs <- num_yes + num_no

    CM = confusionMatrix(table(testData$Attrition,classifications),positive=positive_value)
    acc[j]  = CM$overall[1]
    sens[j] = CM$byClass[1]
    spec[j] = CM$byClass[2]
    pcyes[j] = num_yes / num_obs
    pcno[j]  = num_no / num_obs
  }
  
  # make a matrix with accuracy, sensitivity, and specificity
  knn_stats[i,1] = i
  knn_stats[i,2] = colMeans(acc,na.rm = TRUE)
  knn_stats[i,3] = colMeans(sens,na.rm = TRUE)
  knn_stats[i,4] = colMeans(spec,na.rm = TRUE)
  knn_stats[i,5] = colMeans(pcyes,na.rm = TRUE)
  knn_stats[i,6] = colMeans(pcno,na.rm = TRUE)
}
```

```{r plot_k_stats,eval=TRUE}
stats_frame <- data.frame(knn_stats)
colnames(stats_frame) <- c("k","accuracy","sensitivity","specificity","Perc_Yes","Perc_No")
for_plot <- reshape2::melt(stats_frame,id.var='k')
for_plot %>% ggplot(aes(x=k,y=value,col=variable)) + 
  geom_line() +
  xlab('k') +
  ylab('percent') +
  ggtitle('Performance of Classifier by K')
```

```{r choose_k,eval=TRUE}
combined_stats <- stats_frame %>% 
  mutate(sums = accuracy + sensitivity + specificity) 

max_acc_k <- which.max(knn_stats[,2])
chosen_k  <- max_acc_k

chosen_acc  <- round(100*combined_stats$accuracy[chosen_k])
chosen_sens <- round(100*combined_stats$sensitivity[chosen_k])
chosen_spec <- round(100*combined_stats$specificity[chosen_k])
```

## Select K

max accuracy is at k = `r max_acc_k`.  
Although k of `r max_acc_k` provides the highest accuracy, 
we choose k of `r chosen_k`, because higher and higher k values result in worse specificity.  

At that k, we have  
accuracy of    `r chosen_acc`  
sensitivity of `r chosen_sens`  
specificity of `r chosen_spec`  

```{r cleanup_knn}
rm(knn_stats)
rm(combined_stats)
rm(acc)
rm(sens)
rm(spec)
rm(pcyes)
rm(pcno)
rm(score)
gc()
```

## Repeat classifictiton on entire dataset

```{r knn_eval}
# make a new classifier using all the data
knn_pred <- knn(for_knn[,cols_for_knn],
                for_knn[,cols_for_knn],
                for_knn$Attrition,k=chosen_k)
CM_knn = confusionMatrix(table(as.factor(for_knn$Attrition),knn_pred),positive=positive_value)
knn_acc <- CM_knn$overall[1] # accuracy
knn_sens <- CM_knn$byClass[1] # sensitivity
knn_spec <- CM_knn$byClass[2] # specificity

tmp <- data.frame(knn_pred)
answers <- count(tmp,vars=knn_pred)
num_yes <- as.numeric(answers[2,2])
num_no  <- as.numeric(answers[1,2])
num_obs <- num_yes + num_no
perc_yes <- round(100*num_yes/num_obs)
perc_no  <- round(100*num_no/num_obs)
```

Using the entire dataset,

KNN accuracy turns out to be `r format(100*knn_acc,digits=2)` %.

KNN sensitivity is `r format(100*knn_sens,digits=2)` %.

KNN specificity is `r format(100*knn_spec,digits=2)` %.

## Predict with KNN

Since the KNN classifier does a better job of predicting Attrition, we will
use it for making predictions on the unlabeled data for submission.

```{r knn_pred, eval=TRUE}
# read in the blind test dataset

blind_raw <- read.csv('CaseStudy2CompSetNoAttrition.csv')
blind_test <- my_transform(blind_raw)

if (FALSE) {
  predicted_attrition <- as.factor(knn(for_knn[,cols_for_knn],
                           blind_test[,cols_for_knn],
                           for_knn$Attrition,k=chosen_k))
  str(predicted_attrition)
  summary(predicted_attrition)
} else {
  # rmp try adjusting blind results
  raw_class = as.factor(knn(for_knn[,cols_for_knn],
                          blind_test[,cols_for_knn],
                          for_knn$Attrition,prob=TRUE,k=chosen_k))
  levels(raw_class) <- c("No","Yes")
  
  str(raw_class)
  summary(raw_class)
  
  predicted_attrition = knn_adjust(raw_class, k = chosen_k, num_no = truth.no, num_yes = truth.yes)
  str(predicted_attrition)
  summary(predicted_attrition)
}

summary(predicted_attrition)

for_submission <- cbind(blind_test$ID,as.character(predicted_attrition))
colnames(for_submission) <- c("ID","Attrition")
write.csv(for_submission,file="Case2PredictionsPalmerAttrition.csv",row.names=FALSE,quote=FALSE)
```