---
title: "Naive Bayes"
author: "George M. Ndede"
date: "12/01/2021"
output: html_document
---

```{r setup_nb, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Predict Attrition with Naive Bayes classifier

```{r}
library(e1071)
library(caret)
```

## first determine the true percentage of Yes 

```{r nb_sanity}
truth <- dplyr::count(employee_data,vars=Attrition)
num_yes <- as.numeric(truth[2,2])
num_no  <- as.numeric(truth[1,2])
num_obs <- num_yes + num_no
true_yes_perc <- round(100*num_yes/num_obs)
true_no_perc  <- round(100*num_no/num_obs)
```

True percentage of "yes" values is `r true_yes_perc`  
True percentage of "no"  values is `r true_no_perc` 

```{r nb_parameters}
nb_laplace <- 1
nb_trials <- 100
train_frac <- 0.7
positive_value <- 'Yes'
# for repeatability
set.seed(42)
```

## First try all numeric variables

```{r do_nb_1}
# 
nb_predictors <- c("Age",
                   "Education",
                   "BusinessTravel",
                   "Department",
                   "DistanceFromHome",
                   "EnvironmentSatisfaction",
                   "Gender",
                   "MonthlyIncome",
                   "NumCompaniesWorked",
                   "MaritalStatus",
                   "OverTime",
                   "PercentSalaryHike",
                   "PerformanceRating",
                   "TotalWorkingYears",
                   "TrainingTimesLastYear",
                   "WorkLifeBalance",
                   "YearsAtCompany",
                   "YearsSinceLastPromotion",
                   "YearsWithCurrManager"
                   )

# do the work
acc_nb <- matrix(nrow=nb_trials, ncol = 5)
nobs <- length(employee_data$Attrition)
for (j in 1:nb_trials)
{
  trainIndices <- sample(seq(1:nobs),round(train_frac*nobs))
  trainData <- employee_data[trainIndices,]
  testData <- employee_data[-trainIndices,]
  model_nb <- naiveBayes(trainData[,nb_predictors],trainData$Attrition)
  foo <- predict(model_nb,testData[,nb_predictors])
  tmp <- data.frame(foo)
  answers <- count(tmp,vars=foo)
  num_yes <- as.numeric(answers[2,2])
  num_no  <- as.numeric(answers[1,2])
  num_obs <- num_yes + num_no
  CM_nb <- confusionMatrix(table(predict(model_nb,testData[,nb_predictors]),
                                 testData$Attrition),positive=positive_value)
  acc_nb[j,1] <- CM_nb$overall[1] # accuracy
  acc_nb[j,2] <- CM_nb$byClass[1] # sensitivity
  acc_nb[j,3] <- CM_nb$byClass[2] # specificity
  acc_nb[j,4] <- num_yes / num_obs
  acc_nb[j,5] <- num_no / num_obs

}
mean_stats <- colMeans(acc_nb)
nb_acc <- mean_stats[1]
nb_sens <- mean_stats[2]
nb_spec <- mean_stats[3]
nb_yes_perc <- round(100*mean_stats[4])
nb_no_perc  <- round(100*mean_stats[5])
```

Averaging over `r nb_trials` trials, the Naive Bayes classifier has

accuracy of `r format(100*nb_acc,digits=2)` %

sensitivity of `r format(100*nb_sens,digits=2)` %

specificity of `r format(100*nb_spec,digits=2)` %

Average percentage of "yes" predictions is `r nb_yes_perc` 

Average percentage of "no"  predictions is `r nb_no_perc`  

## Subset the variables and try again

From stepwise variable selection, these are most important (in order)

- TotalWorkingYears
- JobInvolvement
- YearsSinceLastPromotion
- JobSatisfaction
- MaritalStatus
- DistanceFromHome
- BusinessTravel
- WorkLifeBalance
- EnvironmentSatisfaction


```{r do_nb_2}
# predictors from stepwise AIC
nb_predictors_2 <- c("DistanceFromHome",
                   "EnvironmentSatisfaction",
                   "NumCompaniesWorked",
                   "OverTime",
                   "TotalWorkingYears",
                   "TrainingTimesLastYear",
                   "WorkLifeBalance",
                   "YearsSinceLastPromotion",
                   "JobInvolvement",
                   "JobSatisfaction",
                   "MaritalStatus"
                   )
                   

# do the work
acc_nb <- matrix(nrow=nb_trials, ncol = 5)
nobs <- length(employee_data$Attrition)
for (j in 1:nb_trials)
{
  trainIndices <- sample(seq(1:nobs),round(train_frac*nobs))
  trainData <- employee_data[trainIndices,]
  testData <- employee_data[-trainIndices,]
  model_nb <- naiveBayes(trainData[,nb_predictors_2],trainData$Attrition)
  foo <- predict(model_nb,testData[,nb_predictors_2])
  tmp <- data.frame(foo)
  answers <- count(tmp,vars=foo)
  num_yes <- as.numeric(answers[2,2])
  num_no  <- as.numeric(answers[1,2])
  num_obs <- num_yes + num_no
  CM_nb <- confusionMatrix(table(predict(model_nb,testData[,nb_predictors_2]),
                                 testData$Attrition),positive=positive_value)
  acc_nb[j,1] <- CM_nb$overall[1] # accuracy
  acc_nb[j,2] <- CM_nb$byClass[1] # sensitivity
  acc_nb[j,3] <- CM_nb$byClass[2] # specificity
  acc_nb[j,4] <- num_yes / num_obs
  acc_nb[j,5] <- num_no / num_obs

}
mean_stats <- colMeans(acc_nb)
nb_acc <- mean_stats[1]
nb_sens <- mean_stats[2]
nb_spec <- mean_stats[3]
nb_yes_perc <- round(100*mean_stats[4])
nb_no_perc  <- round(100*mean_stats[5])

```

Using a subset of predictors ()

Averaging over `r nb_trials` trials, the Naive Bayes classifier has

accuracy of `r format(100*nb_acc,digits=2)` %

sensitivity of `r format(100*nb_sens,digits=2)` %

specificity of `r format(100*nb_spec,digits=2)` %

Average percentage of "yes" predictions is `r nb_yes_perc` 

Average percentage of "no"  predictions is `r nb_no_perc`  

## Naive Bayes does a moderately accurate job, but sensitivity is poor.
